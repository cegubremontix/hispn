{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaling Distributed Machine Learning with the Parameter Server.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbremont/Notas/blob/main/Papers/Computacion/Scaling_Distributed_Machine_Learning_with_the_Parameter_Server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUWWkQ-hhJZQ"
      },
      "source": [
        "We propose a parameter server framework for distributed\n",
        "machine learning problems. Both data and workloads\n",
        "are distributed over worker nodes, while the server nodes\n",
        "maintain globally shared parameters, represented as dense\n",
        "or sparse vectors and matrices. The framework manages\n",
        "asynchronous data communication between nodes, and\n",
        "supports flexible consistency models, elastic scalability,\n",
        "and continuous fault tolerance.\n",
        "To demonstrate the scalability of the proposed frame-\n",
        "work, we show experimental results on petabytes of real\n",
        "data with billions of examples and parameters on prob-\n",
        "lems ranging from Sparse Logistic Regression to Latent\n",
        "Dirichlet Allocation and Distributed Sketching."
      ]
    }
  ]
}