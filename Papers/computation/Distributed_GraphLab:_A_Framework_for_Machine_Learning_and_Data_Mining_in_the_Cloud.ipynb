{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distributed GraphLab: A Framework for Machine Learning and Data Mining in the Cloud.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbremont/Notas/blob/main/Papers/Computacion/Distributed_GraphLab%3A_A_Framework_for_Machine_Learning_and_Data_Mining_in_the_Cloud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRWUyijQgetD"
      },
      "source": [
        "While high-level data parallel frameworks, like MapReduce, sim-\n",
        "plify the design and implementation of large-scale data processing\n",
        "systems, they do not naturally or efficiently support many important\n",
        "data mining and machine learning algorithms and can lead to ineffi-\n",
        "cient learning systems. To help fill this critical void, we introduced\n",
        "the GraphLab abstraction which naturally expresses asynchronous,\n",
        "dynamic, graph-parallel computation while ensuring data consis-\n",
        "tency and achieving a high degree of parallel performance in the\n",
        "\n",
        "shared-memory setting. In this paper, we extend the GraphLab\n",
        "framework to the substantially more challenging distributed setting\n",
        "while preserving strong data consistency guarantees.\n",
        "We develop graph based extensions to pipelined locking and data\n",
        "versioning to reduce network congestion and mitigate the effect of\n",
        "network latency. We also introduce fault tolerance to the GraphLab\n",
        "abstraction using the classic Chandy-Lamport snapshot algorithm\n",
        "and demonstrate how it can be easily implemented by exploiting\n",
        "the GraphLab abstraction itself. Finally, we evaluate our distributed\n",
        "implementation of the GraphLab abstraction on a large Amazon\n",
        "\n",
        "EC2 deployment and show 1-2 orders of magnitude performance\n",
        "gains over Hadoop-based implementations."
      ]
    }
  ]
}