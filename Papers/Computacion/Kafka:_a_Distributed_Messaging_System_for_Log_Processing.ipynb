{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kafka: a Distributed Messaging System for Log Processing.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbremont/Notas/blob/main/Papers/Computacion/Kafka%3A_a_Distributed_Messaging_System_for_Log_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckPYOCXifngv"
      },
      "source": [
        "Log  processing  has  become  a  critical  component  of  the  data \n",
        "pipeline for consumer internet companies. We introduce Kafka, a \n",
        "distributed messaging system that we developed for collecting and \n",
        "delivering high volumes of log data with low latency. Our system \n",
        "incorporates  ideas  from  existing  log  aggregators  and  messaging \n",
        "systems,  and  is  suitable  for  both  offline  and  online  message \n",
        "consumption.  We  made  quite  a  few  unconventional  yet  practical \n",
        "design choices in Kafka to make our system efficient and scalable. \n",
        "Our experimental results show that Kafka has superior \n",
        "performance  when  compared  to  two  popular  messaging  systems. \n",
        "We  have  been  using  Kafka  in  production  for  some  time  and  it  is \n",
        "processing hundreds of gigabytes of new data each day."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Apache Kafka Use Cases: When To Use It & When Not To](https://www.upsolver.com/blog/apache-kafka-use-cases-when-to-use-not)"
      ],
      "metadata": {
        "id": "2zn-3gXf17vw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lkbcfi_kP3r"
      },
      "source": [
        "Distributed messaging system that collect and deliver  high volumes of log data with low latency.\n",
        "\n",
        "- messaging system?\n",
        "- low latency?\n",
        "\n",
        "- What other messaging systems are there?\n",
        "\n",
        "\n",
        "'Log data':\n",
        "- user logings,\n",
        "-  clicks,\n",
        "- \"likes\",\n",
        "- sharing',  \n",
        "- comments,\n",
        "- sysetm utilization,\n",
        "- cpu,\n",
        "- memory,\n",
        "\n",
        "I can play with the idea, recolecting metrics, in my computer, of mouse moves, key press, memory use\n",
        "- And store those as events,\n",
        "\n",
        "\n",
        "Log data is larger then 'real' data.\n",
        "\n",
        "Every day 'China Mobile' collects 5-8TB of various user activity events.\n",
        "\n",
        "Back in the day the system for processing this log information usually scrapce physical log files. Now there are many 'distributed log aggregators' like **Facebook Scribe**, **Yahoo's Data Highway**, **Claudera's Flume**\n",
        "\n",
        "Kafka is bouth \n",
        "- a log agregator,\n",
        "- a messaging system\n",
        "\n",
        "Kafka simplifies infraestruc because it is used for offline and online processing of 'logs events'\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM_vM1dlo6JE"
      },
      "source": [
        "## Related Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Wz1P0To9Tm"
      },
      "source": [
        "JMS: api, features, garantees.\n",
        "- Does it alows to batch events in one request?\n",
        "- What is the distributed support of JMS\n",
        "\n",
        " What is the implementation detail leaks by 'Flume'?\n",
        "\n",
        "Push vs Pull model\n",
        "\n",
        "Why Kafka uses the pull model?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oggy5iYstMM"
      },
      "source": [
        "##  Kafka Architecture and Desigg Principles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYls0KKgKhuG"
      },
      "source": [
        "**Topic**: A stream of messages of a particular type\n",
        "\n",
        "**A Producer**: Can produce publish messages to a topic. The published messages are thenn stored at a set of servers called *brokers*.\n",
        "\n",
        "**A Consumer**: Can subscribe to one or more topics from the brokers, and consume the subscribed messages by pulling data from the brokers.\n",
        "\n",
        "**A partition**: A way to divide the topics into multiple brokers.\n",
        "\n",
        "**Brokers**: Are the servers that store the partitions, serverd both producers and consumers.\n",
        "\n",
        "```java\n",
        "// Sample of producer code\n",
        "\n",
        "producer = new Producer(…);\n",
        " message = new Message(“test message str”.getBytes());\n",
        " set = new MessageSet(message);\n",
        " producer.send(“topic1”, set);\n",
        "\n",
        "```\n",
        "\n",
        "```java\n",
        "// Sample of consumer code\n",
        " streams[] = Consumer.createMessageStreams(“topic1”, 1)\n",
        " for (message : streams[0]) {\n",
        " bytes = message.payload();\n",
        " // do something with the bytes\n",
        " }\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znNSPHWlL8jH"
      },
      "source": [
        "## Efficiency on a Single Partition\n",
        "\n",
        "Desing desitions in Kafka:\n",
        "\n",
        "- Simple Storage: Each partition of a topic correcponds to a logical log. Physically, a log is implemented as a set of segment files of aproximately the same size (e.g 1GB). A message is only exposed to the consumers after it if flushed to disk.\n",
        "\n",
        "- Mesages are **referred by the offset in disk**, not by  messages id.\n",
        "\n",
        "- A consumer always consumes messages from a particular partition sequentially. If the consumer acknoledges a particular mesage offset, it implies that the consumer has recieved al messages prio to that offest in the partition. Behind the scences, the consuemr is issuing asynchonous pull reqeusts to the broker to have a buffer of data read for the application to consume.\n",
        "\n",
        "- Each pull request contains the offset of the message from which the comsumption beings and an acceptable number of bytes to fetch.\n",
        "\n",
        "- Each broker keeps in memory a softted list of offfsets, includign offsets of the first meesage in every segment file. \n",
        "\n",
        "- After a concumer recieves a meesage, it computes the offset of the netxt message to consume and useds it in the next pull request.\n",
        "\n",
        "**Efficient transfer**\n",
        "\n",
        "Although the end Consumer api iterates one message at a time, under the covers, each pull req4uest froma  consumer also retrieves multiple messages up to a certain size, typically hundreads of kilobytes.\n",
        "\n",
        "Another unconventional choice is to avoid explicitely catchign mesages in emory at the Kafka layer. Instead, we rely on the underlying file sytsem page cache. This eliminates double buffering.\n",
        "\n",
        "Since **Kafka** done do cache at the process level, it makes it feasable to implement it in a Vm-based language, since the garbage collection overhead its recuced by using this technique.\n",
        "\n",
        "The network access is optimized for consumers. Kafka is a multi-subscriber system and a single message may be consumed multiple times by different consumer applications.\n",
        "\n",
        "A typical approach to sending bytes froma  local file to a remote socket involves the folowign steps⁉\n",
        "\n",
        "- 1) read the data from the storage media to the page cache in an OS\n",
        "- 2) copy data in the page cache to an application buffer\n",
        "- 3) copy application buffer to another kernell buffer\n",
        "- 4) send the kernel buffer to the socket \n",
        "\n",
        "This gets optimized in linux, because there is a system call *sendfile* that can directly transfer bytes from a file chanell to a socket chaneel.\n",
        "\n",
        "This avoids the two kernel calls.\n",
        "\n",
        "**Stateless broker**\n",
        "\n",
        "The information about how much each consumer has consumed is not maintained by th broker, but by the consumer itself. This makes tricky to delete a message because Kafka dones not know wheter all subscribers have consumed the message. Kafka solves this problem by suigna simple time-based **SLA** for the retention policy. \n",
        "\n",
        "A message is automatically deleted if it has been retained in the broker longer than a certain period, typically 7 days. \n",
        "\n",
        "A consumer can rewind to and old offset and re-consume data. This biolates the common contract of queue, but proves to be essential feature for many consumers. \n",
        "\n",
        "Re-play messages.\n",
        "\n",
        "Note that rewinding a consumer is much easier to\n",
        "support in the pull model than the push model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5Exfz4ldceq"
      },
      "source": [
        " ## Distributed Coordination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJs4yJ6zdfOD"
      },
      "source": [
        "How the producers and the consumers behave in a distributed setting.\n",
        "\n",
        "Each producer can publish a message to either a randomly selected partition or a partition semanticaly determined by a partitioning key and a partitioning function. \n",
        "\n",
        "How the consumers interact with the brokers?\n",
        "\n",
        "Kafka has the concept of *consumer groups*. Each consumer group consists of one or more consumers that jointly consume a set of subscribed topics, ie, each mesasge is delivered to only one of the consumers withing the group.\n",
        "\n",
        "Patition as the smallest unit of parallelism.\n",
        "\n",
        "Not has a \"master\" node, but instead let consumers coordinate among themselves in a decentralized fashion. To faciliate the coordination,  we employ a highly aviable consensus service Zookeeper. Zookeeper has a very simple, file system like **API**. One can create a path, set the value of the path read the value of a path, delete a path, and list the children path, ...\n",
        "\n",
        "Kakfa uses Zookeeper for the followign tasks:\n",
        "- detecting the addition and the removal of brokers and consumers,\n",
        "- triggering a rebalance process in each consumer when the above events happend,\n",
        "- maintaining the consumption relationship and keeping track of th econsumed offset of each partition\n",
        "\n",
        "Specifically, when each broker or consume starts up, it stores tis information ina broker or consumer registry in Zookeeper. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UgdbCsniQxj"
      },
      "source": [
        "## Delivery Guarantees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQZMc_iJiUz7"
      },
      "source": [
        "In general, Kafka only guarantees at-least-once delivery. Exacly-once delivery typically requires two-phase commits.\n",
        "\n",
        "To avoid log corruption, kafka stores CRC for each message in the log."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saskTzoBi4Xr"
      },
      "source": [
        "## Kafka Usage at LinkedIn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p1px-OvjDFj"
      },
      "source": [
        "Instrumenting producers\n",
        "\n",
        "Apache Hadoop\n",
        "\n",
        "MapReduce\n",
        "\n",
        "Avro as a serialization protocol because supports schema evolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1hDmc93kHPd"
      },
      "source": [
        "## Experimental Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ZBuVJQkKXM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faViCJBjkLOv"
      },
      "source": [
        "## Conclusion and Feture Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iACj4DShkPg9"
      },
      "source": [
        "Like other messaging sytems, Kafka employs a pull-based comsumption model that allows an application to consume data at its own rate and rewidn the comsumption whenever needed.  By focusing on log processing paplication, Kafka achieves much higher throughput than conventional messaging systems. It also provided integrate distributed support and can scale out. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA5A5d-tl181"
      },
      "source": [
        "Todo:\n",
        "\n",
        "- Try to use kafka,\n",
        "- Try to profile kafka,\n",
        "- Try to reconsume messages,\n",
        "- Ankify,\n",
        "- Readread,\n",
        "- Try to replicate kafka results.\n",
        "\n"
      ]
    }
  ]
}