<a href="https://colab.research.google.com/github/dbremont/Notas/blob/main/Ingenerare/Computational_Learning_Articles.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

[firmai/financial-machine-learning: A curated list of practical financial machine learning tools and applications.](https://github.com/firmai/financial-machine-learning)

[huseinzol05/Stock-Prediction-Models: Gathers machine learning and deep learning models for Stock forecasting including trading bots and simulations](https://github.com/huseinzol05/Stock-Prediction-Models)

[ML-From-Scratch](https://github.com/eriklindernoren/ML-From-Scratch)

[Character-level text generation with LSTM](https://keras.io/examples/generative/lstm_character_level_text_generation/)

[kootenpv/whereami: Uses WiFi signals and machine learning to predict where you are](https://github.com/kootenpv/whereami)

[Instruction THroughput Estimator using MAchine Learning (ITHEMAL)](http://3.18.198.23/predict)

[A friendly introduction to Principal Component Analysis | peterbloem.nl](http://peterbloem.nl/blog/pca)

[Transformers from scratch | peterbloem.nl](http://peterbloem.nl/blog/transformers)

- [Loss Landscape Explorer | Explore real loss landscapes of deep learning optimization processes](https://losslandscape.com/explorer)

* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

* [The PGM-index](https://pgm.di.unipi.it/)

* [TrueSkill -  Computing Your Skill](http://www.moserware.com/2010/03/computing-your-skill.html)

* [My First Year as a Freelance AI Engineer](http://masatohagiwara.net/202002-my-first-year-as-a-freelance-ai-engineer.html)

* [Backpropagation](https://cs231n.github.io/optimization-2/)

* [Common data model mistakes made by startups](https://www.metabase.com/learn/data-diet/analytics/data-model-mistakes.html)

* [Neural Algorithms Reading Group](https://brabeeba.github.io/neuralReadingGroup/index.html)

* [Machine learning](https://en.wikipedia.org/wiki/Machine_learning)

* [What data on myself I collect and why? | beepb00p](https://beepb00p.xyz/my-data.html#what)

* [What does it mean for the training data to be generated by a probability distribution over datasets](https://stats.stackexchange.com/questions/320375/what-does-it-mean-for-the-training-data-to-be-generated-by-a-probability-distrib)


* [Multimodal Neurons in Artificial Neural Networks](https://openai.com/blog/multimodal-neurons/)

* [Neural Networks 1](https://cs231n.github.io/neural-networks-1/)

* [Neural Networks Case Study](https://cs231n.github.io/neural-networks-case-study/)

* [Nonlinear system](https://en.wikipedia.org/wiki/Nonlinear_system)

* [Numerical control](https://en.wikipedia.org/wiki/Numerical_control)

* [Optimization 1](https://cs231n.github.io/optimization-1/)

* [Point estimation](https://en.wikipedia.org/wiki/Point_estimation)

* [Predictive analytics](https://en.wikipedia.org/wiki/Predictive_analytics)

* [How to fit an elephant](https://www.johndcook.com/blog/2011/06/21/how-to-fit-an-elephant/)


* [Markov chain](https://en.wikipedia.org/wiki/Markov_chain)

* [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process)

* [Mathematical problem](https://en.wikipedia.org/wiki/Mathematical_problem)

* [Mathematical statistics](https://en.wikipedia.org/wiki/Mathematical_statistics)

* [Mathematics of Sudoku](https://en.wikipedia.org/wiki/Mathematics_of_Sudoku)

* [Limits of statistics](https://www.johndcook.com/blog/2012/09/07/limits-of-statistics/)

* [Linear Classification](https://cs231n.github.io/linear-classify/)

- [Algorithmic Information Theory](http://www.hutter1.net/ait.htm)

- [Deep reinforcement learning will transform manufacturing as we know it](https://techcrunch.com/2021/06/17/deep-reinforcement-learning-will-transform-manufacturing-as-we-know-it/)
  - [Discussion](https://news.ycombinator.com/item?id=27557856)

- [t-statistic](https://en.wikipedia.org/wiki/T-statistic)

- [Understanding p-values Through Simulations](https://rpsychologist.com/pvalue/)

- [Understanding Q-Q Plots](https://data.library.virginia.edu/understanding-q-q-plots/)

- [Student's t-distribution](https://en.wikipedia.org/wiki/Student's_t-distribution)

- [Summary statistics](https://en.wikipedia.org/wiki/Summary_statistics)

- [Survivorship bias](https://en.wikipedia.org/wiki/Survivorship_bias)

- [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)

- [Standard deviation](https://en.wikipedia.org/wiki/Standard_deviation)

- [Statistical dispersion](https://en.wikipedia.org/wiki/Statistical_dispersion)

- [Why I've lost faith in p values](https://lucklab.ucdavis.edu/blog/2018/4/19/why-i-lost-faith-in-p-values)

- [What is P-Value? – Understanding the meaning, math and methods](https://www.machinelearningplus.com/what-is-p-value/)

- [Robust statistics](https://en.wikipedia.org/wiki/Robust_statistics)

- [Simpson's paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox)

- [Skewness](https://en.wikipedia.org/wiki/Skewness)

- [Open-high-low-close chart](https://en.wikipedia.org/wiki/Open-high-low-close_chart)

- [P Values](https://www.statsdirect.com/help/basics/p_values.htm)

- [p-values are inconsistent](https://www.johndcook.com/blog/2010/03/03/p-values-are-inconsistent/)

- [How to test my data against an specific normal distribution?](https://stats.stackexchange.com/questions/56106/how-to-test-my-data-against-an-specific-normal-distribution)

- [Moving Average](https://en.wikipedia.org/wiki/Moving_average)

- [Misinterpretations and misuses of p-values](https://web.ma.utexas.edu/users/mks/statmistakes/misinterppvalues.html)

* [Kolmogorov–Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test)

- [Ecological fallacy](https://en.wikipedia.org/wiki/Ecological_fallacy)

- [Good, mediocre, and bad p-values](https://statmodeling.stat.columbia.edu/2015/04/30/good-mediocre-bad-p-values/)

* [Kernel (statistics)](https://en.wikipedia.org/wiki/Kernel_(statistics))

- [Deviation (statistics)](https://en.wikipedia.org/wiki/Deviation_(statistics))

- [A Litany of Problems With p-values](https://www.fharrell.com/post/pval-litany/)

- [Accuracy and precision](https://en.wikipedia.org/wiki/Accuracy_and_precision)

- [Data-generating probability distribution, probability distribution of a dataset, in ML](https://datascience.stackexchange.com/questions/54346/data-generating-probability-distribution-probability-distribution-of-a-dataset)
  - How can i used a  probability distribution to generate data.

- [Explaining p-values with puppies](https://hackernoon.com/explaining-p-values-with-puppies-af63d68005d0)

- [Friends don’t let friends calculate p-values (without fully understanding them)](http://www.scottbot.net/HIAL/index.html@p=24697.html)

- [Generative model](https://en.wikipedia.org/wiki/Generative_model)

- [Statistical model](https://en.wikipedia.org/wiki/Statistical_model)

- [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet)

- [Average](https://en.wikipedia.org/wiki/Average)

- [Average absolute deviation](https://en.wikipedia.org/wiki/Average_absolute_deviation)

- [Box plot](https://en.wikipedia.org/wiki/Box_plot)

- [Coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation)

* [Statistics (scipy.stats)](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html#t-test-and-ks-test)

* [scipy.stats.ttest_1samp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html)

* [p-value](https://en.wikipedia.org/wiki/P-value)

* [Exclusion of the null hypothesis](https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis)

* [Standard error](https://en.wikipedia.org/wiki/Standard_error)

* [t-statistic](https://en.wikipedia.org/wiki/T-statistic)

* [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)

* [Standard deviation](https://en.wikipedia.org/wiki/Standard_deviation)

* [Log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)

* [Fat-tailed distribution](https://en.wikipedia.org/wiki/Fat-tailed_distribution)

* [Taleb distribution](https://en.wikipedia.org/wiki/Taleb_distribution)

* [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution)

* [Forecasting at Uber: An Introduction](https://eng.uber.com/forecasting-introduction/)

* [Introducing Ludwig, a Code-Free Deep Learning Toolbox](https://eng.uber.com/introducing-ludwig/)

* [Why are neural networks so powerful?](https://towardsdatascience.com/why-are-neural-networks-so-powerful-bc308906696c)

* [EigenGame maps out a new approach to solve fundamental ML problems](https://deepmind.com/blog/article/EigenGame)

* [An Introduction to Knowledge Graphs](http://ai.stanford.edu/blog/introduction-to-knowledge-graphs/)

* [Resources | The Little Dataset](https://thelittledataset.com/data_code/)

* [Knowledge Graph](https://en.wikipedia.org/wiki/Knowledge_graph)

* [Andrew Ng X-Rays the AI Hype](https://arstechnica.com/gadgets/2021/05/apple-hires-yet-another-ex-google-ai-leader/)

* [Hopfield Networks is All You Need](http://justinjaffray.com/query-engines-push-vs.-pull/)

* [Principal Component Analysis](https://setosa.io/ev/principal-component-analysis/)

* [ML beyond Curve Fitting: An Intro to Causal Inference and do-Calculus](https://www.inference.vc/untitled/)

* [No free lunch theorem](https://en.wikipedia.org/wiki/No_free_lunch_theorem)

* [Automated machine learning (AutoML)](https://en.wikipedia.org/wiki/Automated_machine_learning)

* [Search: Query Matching via Lexical, Graph, and Embedding Methods](https://eugeneyan.com/writing/search-query-matching/)

* [Geometric foundations of Deep Learning](https://towardsdatascience.com/geometric-foundations-of-deep-learning-94cdd45b451d)

* [A recommender system for scientific papers](https://statmodeling.stat.columbia.edu/2021/04/16/a-recommender-system-for-scientific-papers/)

* [Machine Learning 101](https://docs.google.com/presentation/d/1kSuQyW5DTnkVaZEjGYCkfOxvzCqGEFzWBy4e9Uedd9k/preview?pru=AAABdHL9X3A*1T5cE4vF1wtY7TY9Pe5d5A&slide=id.g168a3288f7_0_58)

* [Brian Kihoon Lee -  My Path to Machine Learning](https://www.moderndescartes.com/essays/my_ml_path/)

* [How I taught myself Deep Learning: Vanilla NNs](https://www.kaggle.com/andradaolteanu/how-i-taught-myself-deep-learning-vanilla-nns)

* [Why is machine learning 'hard'?](http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html)

* [The Data Science Tree of Knowledge - What is Data Science and How to Educate Data Scientists](https://pabloinsente.github.io/ds-tree-knowledge)

* Machine Learning Crash course
  - [Part 1 - The basics of machine learning - regression, cost functions, and gradient descent.](https://ml.berkeley.edu/blog/posts/crash-course/part-1/)
  - [Part 2 - Perceptrons, logistic regression, and SVMs.](https://ml.berkeley.edu/blog/posts/crash-course/part-2/)
  - [Part 3 - Neural networks.](https://ml.berkeley.edu/blog/posts/crash-course/part-3/)
  - [Part 4 - The Bias-Variance Dilemma.](https://ml.berkeley.edu/blog/posts/crash-course/part-4/)
  - [Part 5 - Decision trees and ensemble models.](https://ml.berkeley.edu/blog/posts/crash-course/part-5/)

* [Python Numpy Tutorial](https://cs231n.github.io/python-numpy-tutorial/)
* [NumPy Fundamentals for Data Science and Machine Learning](https://pabloinsente.github.io/intro-numpy-fundamentals)

* [Google Colab Tips for Power Users](https://amitness.com/2020/06/google-colaboratory-tips)


* [An introduction to reinforcement learning](https://colab.research.google.com/github/psc-g/intro_to_rl/blob/master/Introduction_to_reinforcement_learning.ipynb)

* [Using NLP to allow for flatter organizations](https://middle-out.io/posts/nlp_orgs) 

* [Universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)

* [Multilayer perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)


* [Modern Deep Learning Techniques Applied to Natural Language Processing ](https://nlpoverview.com/)


* [Backpropagation](https://brilliant.org/wiki/backpropagation/)


* [Statistical model](https://en.wikipedia.org/wiki/Statistical_model)

* [Statistics](https://en.wikipedia.org/wiki/Statistics)

* [User modeling](https://en.wikipedia.org/wiki/User_modeling)

* [Introduction to Bayesian Inference](https://www.kaggle.com/philippsinger/introduction-to-bayesian-inference)

* [Approaching (Almost) Any NLP Problem on Kaggle](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle)


* [Pattern recognition](https://en.wikipedia.org/wiki/Pattern_recognition)

* (D) [AI Data Architecture](http://0.0.0.0:8000/2%20Aprendisaje%20Automatico/Articulos/AI%20Data%20Architecture.pdf)

* (D) [Artificial Intelligence and Machine Learning](http://0.0.0.0:8000/2%20Aprendisaje%20Automatico/Articulos/Artificial%20Intelligence%20and%20Machine%20Learning.pdf)


* [A simple solution for monitoring ML systems](https://www.jeremyjordan.me/ml-monitoring/)

* [What does a data-generating process (DGP) actually mean?](https://stats.stackexchange.com/questions/443320/what-does-a-data-generating-process-dgp-actually-mean)

* [Simple PyTorch Transformer Example with Greedy Decoding](https://colab.research.google.com/drive/1swXWW5sOLW8zSZBaQBYcGQkQ_Bje_bmI#scrollTo=b6eCpl-ne7tj)

* [The limitations of deep learning](https://blog.keras.io/the-limitations-of-deep-learning.html)

* [The future of deep learning](https://blog.keras.io/the-future-of-deep-learning.html)

* [Baselines ML](https://madewithml.com/courses/applied-ml/baselines/)


* [What does a data-generating process (DGP) actually mean?](https://stats.stackexchange.com/questions/443320/what-does-a-data-generating-process-dgp-actually-mean)


* [Question answering](https://en.wikipedia.org/wiki/Question_answering)

* [Can I use Deep Learning for that?](https://marksaroufim.medium.com/can-deep-learning-solve-my-problem-a-type-theoretic-heuristic-e57f4d1658f)

* [Inference engine](https://en.wikipedia.org/wiki/Inference_engine)

* [5 Open Problems In NLP](https://deeps.site/blog/2019/09/09/nlp-problems/)

* [A Computational Model for Intelligent Manufacturin](https://industrytoday.com/a-computational-model-for-intelligent-manufacturing/)

* [A top-down, practical guide to learn AI, Deep learning and Machine Learning](https://github.com/emilwallner/How-to-learn-Deep-Learning)

* [AMA: Michael I Jordan](https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/)

* [An Introduction to Hierarchical Modeling](http://mfviz.com/hierarchical-models/)

* [An Overview of Deep Learning for Curious People](https://lilianweng.github.io/lil-log/2017/06/21/an-overview-of-deep-learning.html)

* [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/)

* [Analyze your WhatsApp Chat](https://community.wolfram.com/groups/-/m/t/2063174)

* [Artificial Neural Nets Finally Yield Clues to How Brains Learn](https://www.quantamagazine.org/artificial-neural-nets-finally-yield-clues-to-how-brains-learn-20210218/)

* [Attention and Memory in Deep Learning and NLP](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)

* [Auto training and fast deployement for state-of-the-art NLP models](https://huggingface.co/autonlp)

* [Automating My Job with GPT-3](https://blog.seekwell.io/gpt3)

* [Cancer can be precisely diagnosed using a urine test with artificial intelligence](https://phys.org/news/2021-01-cancer-precisely-urine-artificial-intelligence.html)

* [Cannes: How ML saves us $1.7M a year on document previews](https://dropbox.tech/machine-learning/cannes--how-ml-saves-us--1-7m-a-year-on-document-previews)


* [Categorical Foundations of Gradient-Based Learning](https://www.brunogavranovic.com/posts/2021-03-03-Towards-Categorical-Foundations-Of-Neural-Networks.html)

* [Causal model](https://en.wikipedia.org/wiki/Causal_model)

* [Cellular automaton](https://en.wikipedia.org/wiki/Cellular_automaton)

* [Centrifugal governor](https://en.wikipedia.org/wiki/Centrifugal_governor)

* [Ceteris paribus](https://en.wikipedia.org/wiki/Ceteris_paribus)

* [Color model](https://en.wikipedia.org/wiki/Color_model)

* [Cheap PCB story](https://github.com/hardenedlinux/cheap-pcb/blob/main/cheap-pcb-story.md)

* [Chinese restaurant process](https://en.wikipedia.org/wiki/Chinese_restaurant_process)

* [CLIP: Connecting Text and Images](https://openai.com/blog/clip/)

* [Relations and differences between time-series analysis and statistical signal processing? - Cross Validated](https://stats.stackexchange.com/questions/52270/relations-and-differences-between-time-series-analysis-and-statistical-signal-pr)
* [Keynesian beauty contest](https://en.wikipedia.org/wiki/Keynesian_beauty_contest)
* [Time series](https://en.wikipedia.org/wiki/Time_series)
* [What's the point of time series analysis?](https://stats.stackexchange.com/questions/367590/whats-the-point-of-time-series-analysis)
* [How to Write a Neurips Paper 1. voodoo rituals to procure research… | by Evan Pu | Jan, 2021 | Medium](https://evanthebouncy.medium.com/how-to-write-a-neurips-paper-1-5b25a64d1d03)
* [Machine Learning: The Great Stagnation - Breaking the Stagnation](https://marksaroufim.substack.com/p/machine-learning-the-great-stagnation)
* [The Sovereign Student - Breaking the Stagnation](https://marksaroufim.substack.com/p/the-sovereign-student)
* [No BS Machine Learning News - Breaking the Stagnation](https://marksaroufim.substack.com/p/coming-soon)

* [Word Embeddings: The Good, the Bad, and the Ugly](https://medium.com/ontologik/word-embeddings-the-good-the-bad-and-the-ugly-ad67ada0e688)

* [NLU and Natural Language Interfaces to Databases](https://medium.com/ontologik/nlu-and-natural-language-interfaces-to-databases-d04c3f032949)

* [Data, Information, Knowledge, and GPT-3](https://medium.com/ontologik/data-information-knowledge-and-gpt-3-5e422107b46b)

* [Semantics, Ambiguity, and the role of Probability in NLU](https://medium.com/ontologik/semantics-ambiguity-and-the-role-of-probability-in-nlu-e8e92fc7e8ed)

* [Language & Cognition: re-reading Jerry Fodor](https://medium.com/ontologik/language-cognition-re-reading-jerry-fodor-53e98c9933f4)

* [What we learn vs. what we know](https://medium.com/ontologik/what-we-learn-vs-what-we-know-1569e3fac3c9)

* [Time to put an end to BERTology (or, ML/DL is not even relevant to NLU)](https://medium.com/ontologik/time-to-put-an-end-to-bertology-or-ml-dl-is-not-even-relevant-to-nlu-e5ba6fc53403)

* [In NLU, you ignore intenSion at your peril](https://medium.com/ontologik/in-nlu-you-ignore-intension-at-your-peril-dd173670660d)

* [Cognitive and Universal Primitives of the Language of Thought](https://medium.com/ontologik/cognitive-and-universal-primitives-of-the-language-of-thought-4be67096f9bc)

* [Memorizing vs. Understanding (read: Data vs. Knowledge)](https://medium.com/ontologik/memorizing-vs-understanding-read-data-vs-knowledge-d27c5c756740)

* [How the Tech Giants are Hampering Progress in Artificial Intelligence and Cognitive Computing](https://medium.com/ontologik/how-the-tech-giants-are-hampering-progress-in-artificial-intelligence-and-cognitive-computing-6a7ece7c9623)

* [Learning from Data/Observations is Overrated](https://medium.com/ontologik/learning-from-data-observations-is-overrated-329a20d9fac1)

* [The ‘transfer learning’ problem in DL: a NN is a hardware implementation of a specific algorithm](https://medium.com/ontologik/the-transferability-problem-in-neural-networks-a-nn-is-just-the-hardware-that-implements-a-6e3e54d00172)

* [An Embarrassing Challenge for So-Called AI ‘Experts’ (of the ML/DL Ilk)](https://medium.com/ontologik/an-embarrassing-challenge-for-so-called-ai-experts-of-the-ml-dl-ilk-430541838d0a)

* [Progressives Beware: Vote for the Neoliberals is a Vote for the Fascists and the Far-Right](https://medium.com/@ontologik/progressives-beware-vote-for-the-neoliberals-is-a-vote-for-the-fascists-and-the-far-right-c87551f6c5b3)

* [No Walls in the Global Village](https://medium.com/platopia/no-walls-in-the-global-village-7968afd9a2a8)

* [NLU is not NLP++](https://medium.com/ontologik/nlu-is-not-nlp-617f7535a92e)

* [Data Science? All we Did is Just Replace the ‘Knowledge Bottleneck’ With a ‘Data Bottleneck’](https://medium.com/ontologik/data-science-all-we-did-is-just-replace-the-knowledge-bottleneck-with-a-data-bottleneck-b40c0e642541)

* [Let’s Not Forget the ‘Science’ in ‘Computer Science’](https://medium.com/ontologik/lets-not-forget-the-science-in-computer-science-239d04f4d9a8)

* [(Almost) Automatic Programming: Recursive Thinking as the Easiest Problem Solving Method](https://medium.com/ontologik/almost-automatic-programming-recursive-thinking-as-the-easiest-problem-solving-method-c4204beb01b0)

* [Why Ambiguity is Necessary, and why Natural Language is not Learnable](https://medium.com/ontologik/why-ambiguity-is-necessary-and-why-natural-language-is-not-learnable-79f0e719ac78)

* [Types, Ontology, and a Solution to the (so-called) Paradox of the Ravens](https://medium.com/ontologik/a-solution-to-the-so-called-paradox-of-the-ravens-defdf1ff9b13)

* [The Unrelenting Ghosts of Fodor and Frege: 4 Technical Reasons why Data-Driven and Machine Learning NLU is a Myth](https://medium.com/ontologik/the-unrelenting-ghosts-of-fodor-and-frege-4-technical-reasons-why-data-driven-and-machine-d72698c22775)

* [A Knowledge Graph?](https://medium.com/ontologik/a-knowledge-graph-981d4b7bc605)

* [Must-read papers on graph neural networks (GNN)](https://github.com/thunlp/GNNPapers)

* [The Unrelenting Ghosts of Fodor and Frege: 4 Technical Reasons why Data-Driven and Machine Learning NLU is a Myth](https://medium.com/@ontologik/the-unrelenting-ghosts-of-fodor-and-frege-4-technical-reasons-why-data-driven-and-machine-c2d02be144e9)

* [(Almost) Automatic Programming: Recursion is a lot easier than you think](https://medium.com/@ontologik/almost-automatic-programming-recursion-is-a-lot-easier-than-you-think-49aa4efb6de)

* [A Solution to the so-called Paradox of the Ravens](https://medium.com/@ontologik/a-solution-to-the-so-called-paradox-of-the-ravens-e257d051d0f0)

* [Are NNs Just Fuzzy Hashtables? A Revealing Experiment on MNIST Data](https://medium.com/@ontologik)

* [You Don't Really Need Another MOOC](https://eugeneyan.com/writing/you-dont-need-another-mooc/)

* [Your Thinking Rate Is Fixed](https://fs.blog/2021/03/thinking-rate-fixed/)

* [Zero to Hero (Machine Learning)](https://www.notes2tree.com/published_tree/?publish_tree=caiMJEsnCQ)



* [Convolution](https://en.wikipedia.org/wiki/Convolution)

* [Cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation)

* [Probabilistic Programming](https://simons.berkeley.edu/sites/default/files/docs/5675/talkprintversion.pdf)

* [Basics statistics](http://www.mit.edu/~6.s085/notes/lecture1.pdf)

* [Confidence intervals and hypothesis tests](http://www.mit.edu/~6.s085/notes/lecture2.pdf)

* [Linear Regression](http://www.mit.edu/~6.s085/notes/lecture3.pdf)

* [Regression Diagnostics and Advanced Regression Topics](http://www.mit.edu/~6.s085/notes/lecture4.pdf)

* [Nonparametric statistics and model selection](http://www.mit.edu/~6.s085/notes/lecture5.pdf)

* [Categorical data](http://www.mit.edu/~6.s085/notes/lecture6.pdf)

* [Experimental Design](http://www.mit.edu/~6.s085/notes/lecture7.pdf)

* [Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites](https://webtransparency.cs.princeton.edu/dark-patterns/)



* [Data-generating probability distribution, probability distribution of a dataset, in ML](https://datascience.stackexchange.com/questions/54346/data-generating-probability-distribution-probability-distribution-of-a-dataset)



* [Declarative Graphing](https://srush.github.io/dex-lang/examples/plotting.html)

* [Deep Learning: Our Miraculous Year 1990-1991](https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)

* [ebhy/budgetml: Deploy a ML inference service on a budget in less than 10 lines of code.](https://github.com/ebhy/budgetml)


* [Feature learning](https://en.wikipedia.org/wiki/Feature_learning)


* [Gary Marcus -  What Nate Silver Gets Wrong](https://www.newyorker.com/books/page-turner/what-nate-silver-gets-wrong)

* [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process)

* [Generalized Language Models](https://lilianweng.github.io/lil-log/2019/01/31/generalized-language-models.html)


* [graph - Generating statistics from Git repository - Stack Overflow](https://stackoverflow.com/questions/1828874/generating-statistics-from-git-repository)

* [Graph theory, graph convolutional networks, knowledge graphs](https://albertazout.substack.com/p/gradient-ascent-10)

* [Graphics processing unit](https://en.wikipedia.org/wiki/Graphics_processing_unit)

* [Graphite - Architecture](https://www.aosabook.org/en/graphite.html)

* [THE UNIVERSAL APPROXIMATION THEOREM FOR NEURAL NETWORKS](https://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)

* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

* [Threat Modeling Manifesto](http://www.threatmodelingmanifesto.org/)

* [Time series](https://en.wikipedia.org/wiki/Time_series#Models)


* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)

* [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)


* [Automated scientific research](https://www.youtube.com/watch?v=uLx0gyhVWDo)

* [Nonparametric statistics](https://en.wikipedia.org/wiki/Nonparametric_statistics)

* [Deep Learning: mathematics and neuroscience](http://cbmm.mit.edu/sites/default/files/publications/Deep%20Learning-%20mathematics%20and%20neuroscience.pdf)

* [What are the connections between machine learning and signal processing?](https://www.quora.com/What-are-the-connections-between-machine-learning-and-signal-processing)

* [The Genesis Enterprise: Taking Artificial Intelligence to another Level via a Computational Account of Human Story Understanding.pdf](https://dspace.mit.edu/bitstream/handle/1721.1/119651/CMHI-Report-1.pdf?sequence=1&isAllowed=y)

* [Frontiers in Natural Language Processing Expert Responses](https://docs.google.com/document/d/18NoNdArdzDLJFQGBMVMsQ-iLOowP1XXDaSVRmYN0IyM/edit#)

* [Statistic](https://en.wikipedia.org/wiki/Statistic)



* [The Fibonacci Sequence as a Functor](https://www.math3ma.com/blog/fibonacci-sequence)

* [Offline Reinforcement Learning:  From Algorithms to Practical Challenges](https://sites.google.com/view/offlinerltutorial-neurips2020/home)

* [Free energy principle](https://en.wikipedia.org/wiki/Free_energy_principle)

* [Markov blanket](https://en.wikipedia.org/wiki/Markov_blanket)

* [Generative model](https://en.wikipedia.org/wiki/Generative_model)

* [Discriminative model](https://en.wikipedia.org/wiki/Discriminative_model)

* [Markov model](https://en.wikipedia.org/wiki/Markov_model)

* [Markov Chains](https://brilliant.org/wiki/markov-chains/)

* [Markov Model of Natural Language](https://www.cs.princeton.edu/courses/archive/spr05/cos126/assignments/markov.html)
* [List of datasets for machine-learning research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research#Signal_data)

* [Inductive bias](https://en.wikipedia.org/wiki/Inductive_bias)

* [Abductive reasoning](https://en.wikipedia.org/wiki/Abductive_reasoning)

* [Inductive reasoning](https://en.wikipedia.org/wiki/Inductive_reasoning)

* [Deductive reasoning](https://en.wikipedia.org/wiki/Deductive_reasoning)

* [Variational inference in Bayesian neural networks](http://krasserm.github.io/2019/03/14/bayesian-neural-networks/)

* [Bayes by Backprop from scratch (NN, classification)](https://gluon.mxnet.io/chapter18_variational-methods-and-uncertainty/bayes-by-backprop.html)

* [World Models Can agents learn inside of their own dreams?](https://worldmodels.github.io/)

* [Andre Gelman -  Reflections on Breiman’s Two Cultures of Statistical Modeling](https://www.youtube.com/watch?v=10OV3mucmqc)

* [GitHub - thunlp/PLMpapers: Must-read Papers on pre-trained language models.](https://github.com/thunlp/PLMpapers)

* [Decoded: GNU coreutils](https://www.maizure.org/projects/decoded-gnu-coreutils/)

* [How Difficult is your Programming Project?](https://www.maizure.org/projects/how-difficult-is-your-programming-project.html)

* [I don't want to learn your garbage query language](https://erikbern.com/2018/08/30/i-dont-want-to-learn-your-garbage-query-language.html)

* [Maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)

* [Statistical Mistakes and How to Avoid Them](https://www.cs.cornell.edu/~asampson/blog/statsmistakes.html)

* [The Control Group Is Out Of Control](https://slatestarcodex.com/2014/04/28/the-control-group-is-out-of-control/)

* [The Control Group Is Out Of Control - HN](https://news.ycombinator.com/item?id=13025496)

* [Road Map for Choosing Between Statistical Modeling and Machine Learning](https://www.fharrell.com/post/stat-ml/)



* [Diffusion of innovations](https://en.wikipedia.org/wiki/Diffusion_of_innovations)

* [Adaptive system](https://en.wikipedia.org/wiki/Adaptive_system)

* [Co-occurrence](https://en.wikipedia.org/wiki/Co-occurrence)

* [Distributional semantics](https://en.wikipedia.org/wiki/Distributional_semantics)

* [Sparse distributed memory](https://en.wikipedia.org/wiki/Sparse_distributed_memory)

* [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance)

* [Information theory](https://en.wikipedia.org/wiki/Information_theory)

* [Exponential distribution - Wikipedia](https://en.wikipedia.org/wiki/Exponential_distribution)



* [Technical Perspective: Why Don't Today's Deep Nets Overfit to Their Training Data?](https://cacm.acm.org/magazines/2021/3/250716-technical-perspective-why-dont-todays-deep-nets-overfit-to-their-training-data/fulltext)

* [Zooko's triangle](https://en.wikipedia.org/wiki/Zooko%27s_triangle)



* [Never a dill moment: Exploiting machine learning pickle files](https://blog.trailofbits.com/2021/03/15/never-a-dill-moment-exploiting-machine-learning-pickle-files/)

* [Density estimation](https://en.wikipedia.org/wiki/Density_estimation)

* [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)

* [* [Predictive Coding has been Unified with Backpropagation](https://www.lesswrong.com/posts/JZZENevaLzLLeC3zn/predictive-coding-has-been-unified-with-backpropagation)](https://wiki.c2.com/?HeroicProgramming)

* [Language model](https://en.wikipedia.org/wiki/Language_model)
